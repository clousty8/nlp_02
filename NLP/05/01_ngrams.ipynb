{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram Language Models\n",
    "\n",
    "We will use the Penn Treebank dataset to train an N-gram language model and then generate text with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32tHz5RpCUxC"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Callable, List\n",
    "\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from torchtext.datasets import PennTreebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = PennTreebank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Let's start by cleaning setting the data as simple list of sentences, and then clean them, removing special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_to_item_dataset(dataset) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Extracting dataset as a simple List.\n",
    "    \"\"\"\n",
    "    output = [data for data in dataset]\n",
    "    return output\n",
    "\n",
    "\n",
    "train = iter_to_item_dataset(train)\n",
    "valid = iter_to_item_dataset(valid)\n",
    "test = iter_to_item_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_CHAR_RE = re.compile(\"\\W+\")\n",
    "MULTI_SPACE_RE = re.compile(\"\\s+\")\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove special characters and lower-case the text\n",
    "    \"\"\"\n",
    "    txt = NON_CHAR_RE.sub(\" \", text.lower())\n",
    "    txt = MULTI_SPACE_RE.sub(\" \", txt)\n",
    "    return txt.strip()\n",
    "\n",
    "\n",
    "def preprocessing(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize the raw text by cleaning it and adding the special start and end\n",
    "    tokens.\n",
    "    \"\"\"\n",
    "    return word_tokenize(clean_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The model is a maximum likelyhood estimator (MLE). It will just count the occurences of N-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbq7PvA4CQ1Z"
   },
   "outputs": [],
   "source": [
    "def train_n_gram_model(\n",
    "    texts: List[str], preprocessing_pipeline: Callable[str, List[str]], n: int\n",
    ") -> MLE:\n",
    "    model = MLE(n)\n",
    "    tokenized_texts = [preprocessing_pipeline(text) for text in texts]\n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, tokenized_texts)\n",
    "    model.fit(train_data, padded_sents)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "model = train_n_gram_model(train, preprocessing, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGdAm10MClcc"
   },
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "Let's see what we can generate with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = [preprocessing(text) for text in test]\n",
    "test_data, padded_test = padded_everygram_pipeline(N, tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(25, text_seed=[\"<s>\", \"the\", \"company\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it's your turn\n",
    "\n",
    "* Try computing perplexity on the test data ([this tutorial](https://www.kaggle.com/alvations/n-gram-language-model-with-nltk) can help)\n",
    "* Look into [other models](https://www.nltk.org/api/nltk.lm.html#module-nltk.lm.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ngrams.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

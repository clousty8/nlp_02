{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alJj72mkrgSD",
   "metadata": {
    "id": "alJj72mkrgSD"
   },
   "source": [
    "# Language model using an RNN\n",
    "\n",
    "This notebook implements a simple RNN language model. Go through it and try understanding the code. If you see an error or something which could be made better, please notice your teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41892ac3",
   "metadata": {
    "id": "41892ac3"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Callable, Dict, List, Tuple\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcba7fdc",
   "metadata": {
    "id": "bcba7fdc"
   },
   "source": [
    "## Text pipeline\n",
    "We will use the Penn TreeBank dataset to train an RNN as a language model. The Penn TreeBank is composed of separate sentences so it will be easier for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106151b8",
   "metadata": {
    "id": "106151b8"
   },
   "outputs": [],
   "source": [
    "train, valid, test = PennTreebank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81fe11-5658-49d1-a28f-9f04870be893",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78563853",
   "metadata": {
    "id": "78563853"
   },
   "outputs": [],
   "source": [
    "def iter_to_item_dataset(dataset) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Extracting dataset as a simple List.\n",
    "    \"\"\"\n",
    "    output = [data for data in dataset]\n",
    "    return output\n",
    "\n",
    "\n",
    "train = iter_to_item_dataset(train)\n",
    "valid = iter_to_item_dataset(valid)\n",
    "test = iter_to_item_dataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imBEViDksf4x",
   "metadata": {
    "id": "imBEViDksf4x"
   },
   "source": [
    "### Data cleaning\n",
    "\n",
    "We apply a very simple data preprocessing with a simple removal of special characters and lower casing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd4d94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47dd4d94",
    "outputId": "7a67017e-2048-4c0c-b337-733cefc090a2"
   },
   "outputs": [],
   "source": [
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "NON_CHAR_RE = re.compile(\"\\W+\")\n",
    "MULTI_SPACE_RE = re.compile(\"\\s+\")\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove special characters and lower-case the text\n",
    "    \"\"\"\n",
    "    txt = NON_CHAR_RE.sub(\" \", text.lower())\n",
    "    txt = MULTI_SPACE_RE.sub(\" \", txt)\n",
    "    return txt.strip()\n",
    "\n",
    "\n",
    "def tokenize_raw(tokenizer: Callable[[str], List[str]], text: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Tokenize the raw text by cleaning it and adding the special start and end\n",
    "    tokens.\n",
    "    \"\"\"\n",
    "    return [START_TOKEN] + tokenizer(clean_text(text)) + [END_TOKEN]\n",
    "\n",
    "\n",
    "def yield_tokens(data):\n",
    "    \"\"\"\n",
    "    Iterating function\n",
    "    \"\"\"\n",
    "    for text in data:\n",
    "        tokens = tokenize_raw(tokenizer, text)\n",
    "        yield tokens\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train), min_freq=3, specials=[UNK_TOKEN, START_TOKEN, END_TOKEN]\n",
    ")\n",
    "vocab.set_default_index(vocab[UNK_TOKEN])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coJZL5CPtgWp",
   "metadata": {
    "id": "coJZL5CPtgWp"
   },
   "source": [
    "For this example, we keep a rather small vocabulary (mostly so we can iterate on the model and try different parameters faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6e51b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fc6e51b",
    "outputId": "faacaa84-c91b-480a-a6f5-5a81b7bfa3e7"
   },
   "outputs": [],
   "source": [
    "class TextPipeline:\n",
    "    \"\"\"\n",
    "    The text pipeline which will take raw text as input and return\n",
    "    tokenized words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, vocab):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._vocab = vocab\n",
    "\n",
    "    def pipeline(self, text):\n",
    "        return self._vocab(tokenize_raw(self._tokenizer, text))\n",
    "\n",
    "\n",
    "text_pipeline = TextPipeline(tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6g1wKkq3wWFz",
   "metadata": {
    "id": "6g1wKkq3wWFz"
   },
   "source": [
    "So what does our data look like when it goes through the pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uGYoXJGhwaCI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGYoXJGhwaCI",
    "outputId": "e5a8b562-a06d-4597-96f5-deac8d2f3fa7"
   },
   "outputs": [],
   "source": [
    "print(train[1])\n",
    "print(text_pipeline.pipeline((train[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diHscguUt3D_",
   "metadata": {
    "id": "diHscguUt3D_"
   },
   "source": [
    "### Dataset and DataLoader\n",
    "PyTorch asks for data being provided through a DataLoader. This Dataset is just a convenience class which wraps the dataset with a `__len__` and a `__getitem__` function.\n",
    "\n",
    "Note that the `__getitem__` function returns  tuple which correspond to the original sentence and a shifted version of it, so that the first element correspond to the next input to the RNN and the second one to the corresponding expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06383878",
   "metadata": {
    "id": "06383878"
   },
   "outputs": [],
   "source": [
    "class LMData(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class wrapping our dataset by adding a `__len__` and a `__getitem__`\n",
    "    function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, vocab, text_pipeline: TextPipeline):\n",
    "        self._data = data\n",
    "        self._vocab = vocab\n",
    "        self._pipeline = text_pipeline\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[List[int], List[int]]:\n",
    "        \"\"\"\n",
    "        Return a tuple corresponding to the current sentence and a left-shifted\n",
    "        version of it.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            torch.tensor(self._pipeline.pipeline(self._data[idx])[:-1]),\n",
    "            torch.tensor(self._pipeline.pipeline(self._data[idx])[1:]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee57cb5",
   "metadata": {
    "id": "5ee57cb5"
   },
   "outputs": [],
   "source": [
    "train_dataset = LMData(train, vocab, text_pipeline)\n",
    "valid_dataset = LMData(valid, vocab, text_pipeline)\n",
    "test_dataset = LMData(test, vocab, text_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KJ7uahx1v68N",
   "metadata": {
    "id": "KJ7uahx1v68N"
   },
   "source": [
    "Let's check again our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meq9G7mZVwL9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "meq9G7mZVwL9",
    "outputId": "58233642-ee83-4fef-9efa-554268917d0d"
   },
   "outputs": [],
   "source": [
    "[vocab.vocab.get_itos()[i] for i in train_dataset[10][0].numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c5c63",
   "metadata": {
    "id": "628c5c63"
   },
   "source": [
    "The DataLoader is used by pyTorch to provide batches to the model. For these batches to be treated in parallel on a GPU, they need to all have the same size. We will simply pad the shorter sentences in each batch with end of sentence token (\"\\</s\\>\").\n",
    "\n",
    "Note that pyTorch provides a mechanism called [\"packing\"](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch) which prevents the useless overhead in forward propagation and remove the padded data from the loss computation (and which is hardly documented)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9541db",
   "metadata": {
    "id": "bc9541db"
   },
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c596bd9",
   "metadata": {
    "id": "4c596bd9"
   },
   "outputs": [],
   "source": [
    "def collate_fn(data: LMData):\n",
    "    \"\"\"\n",
    "    This function is given to the DataLoader. It only pads the input and output\n",
    "    with </s>\n",
    "    \"\"\"\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    lens = [len(sent) for sent, _ in data]\n",
    "    labels = []\n",
    "    padded_inputs = (\n",
    "        torch.ones(len(data), max(lens)).long() * vocab.get_stoi()[END_TOKEN]\n",
    "    )\n",
    "    padded_outputs = (\n",
    "        torch.ones(len(data), max(lens)).long() ** vocab.get_stoi()[END_TOKEN]\n",
    "    )\n",
    "    for i, (input_sent, output_sent) in enumerate(data):\n",
    "        padded_inputs[i, : lens[i]] = torch.LongTensor(input_sent)\n",
    "        padded_outputs[i, : lens[i]] = torch.LongTensor(output_sent)\n",
    "    # Putting data in the right shape\n",
    "    padded_inputs = padded_inputs.transpose(0, 1)\n",
    "    padded_outputs = padded_outputs.transpose(0, 1)\n",
    "\n",
    "    return padded_inputs, padded_outputs, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed0a5c4",
   "metadata": {
    "id": "aed0a5c4"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d0d4f",
   "metadata": {
    "id": "fc2d0d4f"
   },
   "source": [
    "## The model\n",
    "\n",
    "Now we will compose an RNN model composed of:\n",
    "* An embedding layer\n",
    "* An RNN\n",
    "* A fully connected layer which project the hidden layer to the vocabulary size (output logit).\n",
    "\n",
    "At every call, the mode takes the current input and hidden state and return the computed logit and new hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca72bdd",
   "metadata": {
    "id": "3ca72bdd"
   },
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    \"\"\"\n",
    "    A RNN-based class for language modelling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        n_layers: int = 1,\n",
    "        dropout: int = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        # Embedding layer (vocab_size -> embedding_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        # RNN layer (embedding_size -> (output_size, hidden_size)\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, n_layers, dropout=dropout)\n",
    "        # Fully connected layer (hidden_size > vocab_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: Tensor, hidden: Tensor, lens: Tensor):\n",
    "        \"\"\"\n",
    "        The function exectured every time the model is called\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(x)\n",
    "        # To avoid computing the loss and working on padded sequences\n",
    "        # PyTorch provides a mechanism called packing\n",
    "        embeddings = nn.utils.rnn.pack_padded_sequence(embeddings, lens)\n",
    "        output, hidden = self.rnn(embeddings, hidden)\n",
    "        output, output_lens = pad_packed_sequence(output)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Initialize the hidden layer at 0s. There are better ways...\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x2HKvN3nyDXt",
   "metadata": {
    "id": "x2HKvN3nyDXt"
   },
   "source": [
    "Before training, let's check we have access to a GPU. If not (and you are on colab), make sure you enable a GPU in `Runtime>Change runtime type`. You can also train the model on a CPU, but it will take a lot longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S8CI9eP7yVMf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8CI9eP7yVMf",
    "outputId": "1451bd4b-f6a7-41dc-c453-c7ece69ec8e5"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# this should show \"device(type='cuda')\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcfedf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "debcfedf",
    "outputId": "b7fe4b8a-e462-4493-f85f-6f2dac2624b6"
   },
   "outputs": [],
   "source": [
    "model = RNNLM(len(vocab), 16, 256, 64, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wu7QX_xwMJGz",
   "metadata": {
    "id": "wu7QX_xwMJGz"
   },
   "source": [
    "### Training\n",
    "We train the model on our data by trying to predict the actual output and updating weights accordingly.\n",
    "\n",
    "Note that if you are not familiar with pyTorch, the cross-entropy function applies softmax on the logits by itself, so there is no need to apply a softmax function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9adcba8",
   "metadata": {
    "id": "a9adcba8"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_loader,\n",
    "    model: RNNLM,\n",
    "    nb_epochs: int,\n",
    "    learning_rate: float = 0.001,\n",
    "    batch_size: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Our training procedure\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    vocab_size = len(vocab)\n",
    "    # We are using a cross entropy loss function and a RMSprop optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(nb_epochs):\n",
    "        total_loss = 0\n",
    "        nb_batches = 0\n",
    "        for batch, (x, y, lens) in enumerate(train_loader):\n",
    "            # Putting the data on the device (GPU)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Initializing the hidden layer\n",
    "            hidden = model.init_hidden(x.shape[1]).to(device)\n",
    "            # And the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, hidden = model(x, hidden, lens)\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss\n",
    "            nb_batches += 1\n",
    "            if nb_batches % 1000 == 0:\n",
    "                print({\"batch\": nb_batches, \"loss\": total_loss / nb_batches})\n",
    "\n",
    "        print(\n",
    "            {\"epoch\": epoch, \"nb_batches\": nb_batches, \"loss\": total_loss / nb_batches}\n",
    "        )\n",
    "        print(f\"Next learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2394ae66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2394ae66",
    "outputId": "bd8e37bc-f38c-4645-f800-f4b46f6cb212",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(train_loader, model, 3, learning_rate=0.001, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa3696-e3eb-407a-b1a4-49f2c3d8ea3c",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "To predict the next word, we use a random sampling taking into account the probability of each word given by a softmax plugged on top of our logits. We also use temperatures (0 < temperature <= 1) to affect the probabilities. A lower temperature will make the most likely word more likely to be chosen, while a higher temperature will make picks more random (more on that topic in our next course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e089d34",
   "metadata": {
    "id": "4e089d34"
   },
   "outputs": [],
   "source": [
    "def predict(model: RNNFLM, vocab: Vocab, start: int=[START_TOKEN], max_words: int = 25, temperature: int = 1.0):\n",
    "    \"\"\"\n",
    "    Generate a more or less likely sentence from a given start\n",
    "    The temperature parameter is something we'll see in our next lesson\n",
    "    For now think that a lower temperature make the prediction closer to always\n",
    "    choosing the most likely word.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    hidden = model.init_hidden(1).to(device)\n",
    "    current_word = start[-1]\n",
    "    words = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, max_words):\n",
    "            x = torch.tensor(vocab([current_word])).reshape(1, 1).to(device)\n",
    "            y_pred, hidden = model(x, hidden, [1])\n",
    "            # We force the input value until we reach the end of the start list\n",
    "            if i >= len(start):\n",
    "                last_word_logits = y_pred[0][0]\n",
    "                last_word_logits /= temperature\n",
    "                p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "                word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "#                 word_index = np.argmax(p)\n",
    "            else:\n",
    "                word_index = vocab.vocab.get_stoi()[start[i]]\n",
    "            words.append(vocab.vocab.get_itos()[word_index])\n",
    "            if words[-1] == \"</s>\":\n",
    "                break\n",
    "            current_word = words[-1]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfdda3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45cfdda3",
    "outputId": "9e3ffa79-2822-4e54-e496-2abf3364f01f"
   },
   "outputs": [],
   "source": [
    "predict(model, vocab, [START_TOKEN, \"the\", \"company\"], temperature = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8iMycTL5NH38",
   "metadata": {
    "id": "8iMycTL5NH38"
   },
   "source": [
    "## Now it's your turn\n",
    "\n",
    "* Play with the dimensions of the model (embedding, hidden, number of layers, ...) and compare the generated outputs.\n",
    "* Create a class LSTMLM or GRULM which uses an LSTM or a GRU cell and compare the results.\n",
    "* Use the language model to compute the perplexity on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kHIymcPvNubq",
   "metadata": {
    "id": "kHIymcPvNubq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

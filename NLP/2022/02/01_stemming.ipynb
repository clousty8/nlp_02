{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTNGn_9NwgzW"
   },
   "source": [
    "# Stemming\n",
    "\n",
    "Let's see an example of stemming text using [NLTK](https://www.nltk.org/) (Natural Language ToolKit). We will use their SnowballStemmer implementation. The implementation is available online, so if you are curious about how stemming is done in different languages, you can look [here](https://www.nltk.org/api/nltk.stem.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zYDwL5inwK72"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "BGY0wkxVwOfQ",
    "outputId": "99bcb8b9-1b55-4355-b9f2-14134eedfa12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marcvonwyl/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to download a package for word tokenization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "w-J8ad0rxN5I",
    "outputId": "6ae07d2a-2c79-4dcb-9508-c03195273c3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At first, historical linguistics served as the cornerstone of comparative linguistics primarily as a tool for linguistic reconstruction.[5] Scholars were concerned chiefly with establishing language families and reconstructing prehistoric proto-languages, using the comparative method and internal reconstruction.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"At first, historical linguistics served as the cornerstone of comparative linguistics primarily as a tool for linguistic reconstruction.[5] Scholars were concerned chiefly with establishing language families and reconstructing prehistoric proto-languages, using the comparative method and internal reconstruction.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3MSl9i5xV5L"
   },
   "source": [
    "Let's start with the word tokenization. Notice how it cut words and symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "6OuOKkwAxSi2",
    "outputId": "f9e8bdd8-5e9a-45c0-ba37-1bbdce0c7a21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At first , historical linguistics served as the cornerstone of comparative linguistics primarily as a tool for linguistic reconstruction . [ 5 ] Scholars were concerned chiefly with establishing language families and reconstructing prehistoric proto-languages , using the comparative method and internal reconstruction .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESYdyL6Txb76"
   },
   "source": [
    "Rule-based tokenizers have to make implementation choices. For example, not splitting hyphenated words (`proto-languages`) or cutting symbols independently (`[ 5 ]`). Different tokenizers will bring different results. \n",
    "\n",
    "NLTK provides several [word and sentence tokenizers](https://www.nltk.org/api/nltk.tokenize.html).\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Now let's apply the stemming to everything that is composed of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "KTWladxBxaRf",
    "outputId": "322dcb2d-80a7-4684-f7d2-e4d734f0ef51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at first histor linguist serv as the cornerston of compar linguist primarili as a tool for linguist reconstruct 5 scholar were concern chiefli with establish languag famili and reconstruct prehistor use the compar method and intern reconstruct'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re_word = re.compile(r\"^\\w+$\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed = [stemmer.stem(word) for word in word_tokenize(text.lower()) if re_word.match(word)]\n",
    "        \n",
    "\" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYKiWWQrxlbR"
   },
   "source": [
    "Note how the words are simply cut and stemmed. Note that \"were\" didn't change as it does not follow standard stemming rules.\n",
    "\n",
    "Another example with \"went\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MCBsfFB8xjAQ",
    "outputId": "c0bf615d-4a07-4904-cb15-b87a05b3497c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i went to the cinema'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" I went to the cinema\"\n",
    "stemmed = [stemmer.stem(word) for word in word_tokenize(text.lower()) if re_word.match(word)]\n",
    "        \n",
    "\" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed\n",
    "\n",
    "Let's see how fast stemming is on a certain quantity of text. [TorchText](https://pytorch.org/text/stable/index.html) proposes several [datasets](https://pytorch.org/text/stable/datasets.html), including the text-only part of the [Penn treebank](https://pytorch.org/text/stable/datasets.html#penntreebank) containing 42068 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "l6vm2JcpySW1"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import PennTreebank\n",
    "train, valid, test = PennTreebank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 42068/42068 [00:10<00:00, 3991.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "re_word = re.compile(r\"^\\w+$\")\n",
    "nb_unique_token = set()\n",
    "nb_unique_stem = set()\n",
    "for text in tqdm(train, total=len(train)):\n",
    "    for token in word_tokenize(text):\n",
    "        if re_word.match(token):\n",
    "            nb_unique_token.add(token)\n",
    "            nb_unique_stem.add(stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb unique token: 9519 vs nb unique stem: 6151\n"
     ]
    }
   ],
   "source": [
    "print(f\"nb unique token: {len(nb_unique_token)} vs nb unique stem: {len(nb_unique_stem)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mxgZJyQ0Prs"
   },
   "source": [
    "## Going Further\n",
    "\n",
    "NLTK proposes several stemming implementation in several languages. Notably, [this little tutorial](http://www.nltk.org/howto/stem.html) shows how to use the `Snowball stemmer` in several languages. You an also directly look into [NLTK's implementation](https://www.nltk.org/_modules/nltk/stem/snowball.html) of different stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "stemming.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
